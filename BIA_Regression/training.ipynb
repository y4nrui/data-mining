{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LassoCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_df = pd.read_csv('./Data/one_hot_df.csv')\n",
    "label_df = pd.read_csv('./Data/label_df.csv')\n",
    "y = one_hot_df['SalePrice']\n",
    "X = one_hot_df.drop(['SalePrice', 'Unnamed: 0'], axis =1)\n",
    "label_X = label_df.drop(['SalePrice', 'Unnamed: 0'], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CV PREDICTION ERROR FOR MULTIVARIATE LR\n2656881023.263894\n"
    }
   ],
   "source": [
    "lr_model = LinearRegression()\n",
    "metrics = cross_validate(lr_model, X,y,cv = 5, scoring = ('neg_root_mean_squared_error'))\n",
    "lr_cv = -metrics['test_score'].mean()\n",
    "print('CV PREDICTION ERROR FOR MULTIVARIATE LR')\n",
    "print(lr_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9258276520543647"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "lr_model.fit(X,y)\n",
    "lr_model.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression model gives a very high R squared of 0.926, but this is expected because we have included almost all features inside our regression model. Looking for adjusted r squared metric. We have 272 predictor variables in our model. \n",
    "\n",
    "The CV prediction error we are getting is abnormally high. We need to investigate this. Does not make sense, because it is a log(sales price) the RMSE cannot go into the millions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CV PREDICTION ERROR FOR UNTUNED RR\n0.14151972402903668\n"
    }
   ],
   "source": [
    "param_list = []\n",
    "start = 10\n",
    "for i in range(30):\n",
    "    param_list.append(start)\n",
    "    start = start + 0.25\n",
    "\n",
    "parameters = {'alpha': param_list} \n",
    "rr = Ridge()\n",
    "metrics = cross_validate(rr, X, y, cv = 5, scoring = ('neg_root_mean_squared_error'))\n",
    "print('CV PREDICTION ERROR FOR UNTUNED RR')\n",
    "print(-metrics['test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the ridge regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CV PREDICTION ERROR FOR TUNED RR\n0.13646741581659594\n{'alpha': 15.0}\n"
    }
   ],
   "source": [
    "tuned_rr = GridSearchCV(rr, parameters, scoring = 'neg_root_mean_squared_error', cv = 5)\n",
    "tuned_rr.fit(X,y)\n",
    "print('CV PREDICTION ERROR FOR TUNED RR')\n",
    "print(-tuned_rr.best_score_)\n",
    "print(tuned_rr.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the value of alpha that gives us the lowest cross validation prediction error. The alpha "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "SCORE FOR UNTUNED LASSO\n0.3992282792085989\n"
    }
   ],
   "source": [
    "param_list = []\n",
    "start = 0.0001\n",
    "for i in range(30):\n",
    "    param_list.append(start)\n",
    "    start = start + 0.0001\n",
    "parameters2 = {'alpha': param_list}\n",
    "lasso = Lasso()\n",
    "lasso.fit(X,y)\n",
    "metrics = cross_validate(lasso, X,y, cv = 5, scoring = ('neg_root_mean_squared_error'))\n",
    "print('SCORE FOR UNTUNED LASSO')\n",
    "print(-metrics['test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the lasso regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_lasso = GridSearchCV(lasso, parameters2, scoring = 'neg_root_mean_squared_error', cv = 5)\n",
    "tuned_lasso.fit(X,y)\n",
    "best_param = tuned_lasso.best_params_['alpha']\n",
    "lasso_tuned = Lasso(alpha = best_param)\n",
    "lasso_tuned.fit(X,y)\n",
    "metrics = cross_validate(lasso_tuned, X,y, cv = 5, scoring = ('neg_root_mean_squared_error'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of tuned lasso regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "BASED ON CV, WE FOUND OPTIMAL ALPHA TO BE:\n0.0005\n0.13390462926904387\n\n\nFEATURES WITH NON ZERO COEFFICIENTS FOR LASSO\n['LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch', 'MSSubClass_20', 'MSSubClass_50', 'MSZoning_RL', 'LotShape_IR2', 'LotConfig_Corner', 'LotConfig_CulDSac', 'Neighborhood_BrkSide', 'Neighborhood_ClearCr', 'Neighborhood_Crawfor', 'Neighborhood_NoRidge', 'Neighborhood_NridgHt', 'Neighborhood_Somerst', 'Neighborhood_StoneBr', 'Condition1_Norm', 'Condition1_RRAn', 'BldgType_1Fam', 'Exterior1st_BrkFace', 'Exterior1st_MetalSd', 'Exterior1st_Plywood', 'Exterior1st_VinylSd', 'Exterior2nd_VinylSd', 'Exterior2nd_Wd Sdng', 'ExterQual_Gd', 'ExterCond_TA', 'Foundation_PConc', 'BsmtQual_Ex', 'BsmtCond_TA', 'BsmtExposure_Gd', 'BsmtFinType1_ALQ', 'BsmtFinType1_GLQ', 'BsmtFinType2_Unf', 'Heating_GasW', 'HeatingQC_Ex', 'HeatingQC_Gd', 'Electrical_FuseA', 'KitchenQual_Ex', 'Functional_Typ', 'GarageType_Attchd', 'GarageType_Detchd', 'GarageQual_Gd', 'GarageCond_TA', 'PavedDrive_Y', 'MoSold_5', 'MoSold_6', 'MoSold_7', 'SaleCondition_Normal']\n"
    }
   ],
   "source": [
    "print('BASED ON CV, WE FOUND OPTIMAL ALPHA TO BE:')\n",
    "print(best_param)\n",
    "print(-tuned_lasso.best_score_)\n",
    "print()\n",
    "model_coefs = list(lasso_tuned.coef_)\n",
    "\n",
    "good_features = []\n",
    "for i in range(len(model_coefs)):\n",
    "    if model_coefs[i]> 0:\n",
    "        good_features.append(list(X.columns)[i])\n",
    "\n",
    "print('\\n'+'FEATURES WITH NON ZERO COEFFICIENTS FOR LASSO')\n",
    "print(good_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "MSSubClass_20, MSSubClass_50, MSZoning_RL, LotShape_IR2, LotConfig_Corner, LotConfig_CulDSac, Neighborhood_BrkSide, Neighborhood_ClearCr, Neighborhood_Crawfor, Neighborhood_NoRidge, Neighborhood_NridgHt, Neighborhood_Somerst, Neighborhood_StoneBr, Condition1_Norm, Condition1_RRAn, BldgType_1Fam, Exterior1st_BrkFace, Exterior1st_MetalSd, Exterior1st_Plywood, Exterior1st_VinylSd, Exterior2nd_VinylSd, Exterior2nd_Wd Sdng, ExterQual_Gd, ExterCond_TA, Foundation_PConc, BsmtQual_Ex, BsmtCond_TA, BsmtExposure_Gd, BsmtFinType1_ALQ, BsmtFinType1_GLQ, BsmtFinType2_Unf, Heating_GasW, HeatingQC_Ex, HeatingQC_Gd, Electrical_FuseA, KitchenQual_Ex, Functional_Typ, GarageType_Attchd, GarageType_Detchd, GarageQual_Gd, GarageCond_TA, PavedDrive_Y, MoSold_5, MoSold_6, MoSold_7, SaleCondition_Normal,"
    }
   ],
   "source": [
    "for i in good_features:\n",
    "    if '_' in i:\n",
    "        print(i, end = ', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform feature selection based on the coefficients of our lasso regression model, narrowing our features down from 273 to 66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of alpha that gives us the lowest CV prediction error is 0.0004. This alpha value is very low: when alpha is low it means that the result (model coefficients) become similar to that of the linear regression model. Because the penalty that is placed on the coefficients in the optimisation function is lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting selection of features: Based on the one hot encoding, not all were important. For example, for months, if variable was in month 5,6,7 seemed to have an impact. This incentivizes us to think that perhaps doing some feature engineering might improve the results. Change the classes found within some of the features which are categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reworking based on feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CV PREDICTION ERROR FOR NEW FEATURE SET MULTIVARIATE LR\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.13631552802441838"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "new_feature_set = X[good_features]\n",
    "lr_model_selected = LinearRegression()\n",
    "metrics = cross_validate(lr_model_selected, new_feature_set, y, cv = 5,scoring = ('neg_root_mean_squared_error'))\n",
    "print('CV PREDICTION ERROR FOR NEW FEATURE SET MULTIVARIATE LR')\n",
    "-metrics['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a linear regression model based on feature selection with our lasso regression model helps us derive a significantly lower CV prediction error at 0.136350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_list = []\n",
    "start = 4\n",
    "for i in range(30):\n",
    "    param_list.append(start)\n",
    "    start = start + 0.1\n",
    "#     start = start +\n",
    "\n",
    "parameters = {'alpha': param_list} \n",
    "rr = Ridge()\n",
    "metrics = cross_validate(rr, X[good_features], y, cv = 5, scoring = ('neg_root_mean_squared_error'))\n",
    "print('CV PREDICTION ERROR FOR UNTUNED RR')\n",
    "print(-metrics['test_score'].mean())\n",
    "\n",
    "tuned_rr = GridSearchCV(rr, parameters, scoring = 'neg_root_mean_squared_error', cv = 5)\n",
    "tuned_rr.fit(X[good_features],y)\n",
    "print('CV PREDICTION ERROR FOR TUNED RR')\n",
    "print(-tuned_rr.best_score_)\n",
    "print(tuned_rr.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see little difference between our original ridge regression model and the new ridge regression model. CV prediction model is very similar, slight improvement when we incorporate feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = []\n",
    "start = 0.0001\n",
    "for i in range(30):\n",
    "    param_list.append(start)\n",
    "    start = start + 0.0001\n",
    "parameters2 = {'alpha': param_list}\n",
    "lasso = Lasso()\n",
    "lasso.fit(new_feature_set,y)\n",
    "metrics = cross_validate(lasso, new_feature_set,y, cv = 5, scoring = ('neg_root_mean_squared_error'))\n",
    "print('SCORE FOR UNTUNED LASSO')\n",
    "print(-metrics['test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_lasso = GridSearchCV(lasso, parameters2, scoring = 'neg_root_mean_squared_error', cv = 5)\n",
    "tuned_lasso.fit(new_feature_set,y)\n",
    "best_param = tuned_lasso.best_params_['alpha']\n",
    "lasso_tuned = Lasso(alpha = best_param)\n",
    "lasso_tuned.fit(X,y)\n",
    "metrics = cross_validate(lasso_tuned, new_feature_set,y, cv = 5, scoring = ('neg_root_mean_squared_error'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BASED ON CV, WE FOUND OPTIMAL ALPHA TO BE:')\n",
    "print(best_param)\n",
    "print(-tuned_lasso.best_score_)\n",
    "print()\n",
    "model_coefs = list(lasso_tuned.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that the CV prediction error does not change after we include feature selection. This is because of the mechanism of the lasso regression, whereby insignificant variables are already shrunk towards 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': 16, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'oob_score': True, 'random_state': None, 'verbose': 0, 'warm_start': False}\n\n This is the oob score:  0.8067589925893359\n"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 500, max_leaf_nodes = 16, n_jobs = -1, oob_score = True, bootstrap = True)\n",
    "rf.fit(new_feature_set, y)\n",
    "print(rf.get_params())\n",
    "print('\\n This is the oob score: ', rf.oob_score_)\n",
    "rf_features = []\n",
    "\n",
    "for name, score in zip(list(label_X.columns),rf.feature_importances_):\n",
    "    if score > 0:\n",
    "#         print(name,score)\n",
    "        rf_features.append((name,score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 500, max_leaf_nodes = 16, n_jobs = -1, oob_score = True, bootstrap = True)\n",
    "rf.fit(new_feature_set, y)\n",
    "print(rf.get_params())\n",
    "print('\\n This is the oob score: ', rf.oob_score_)\n",
    "rf_features = []\n",
    "\n",
    "for name, score in zip(list(label_X.columns),rf.feature_importances_):\n",
    "    if score > 0:\n",
    "#         print(name,score)\n",
    "        rf_features.append((name,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_features.sort(key = lambda x:x[1], reverse = True)\n",
    "# rf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "LASSO REGRESSION FEATURES SELECTED: \n ['LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch', 'MSSubClass_20', 'MSSubClass_50', 'MSZoning_RL', 'LotShape_IR2', 'LotConfig_Corner', 'LotConfig_CulDSac', 'Neighborhood_BrkSide', 'Neighborhood_ClearCr', 'Neighborhood_Crawfor', 'Neighborhood_NoRidge', 'Neighborhood_NridgHt', 'Neighborhood_Somerst', 'Neighborhood_StoneBr', 'Condition1_Norm', 'Condition1_RRAn', 'BldgType_1Fam', 'Exterior1st_BrkFace', 'Exterior1st_MetalSd', 'Exterior1st_Plywood', 'Exterior1st_VinylSd', 'Exterior2nd_VinylSd', 'Exterior2nd_Wd Sdng', 'ExterQual_Gd', 'ExterCond_TA', 'Foundation_PConc', 'BsmtQual_Ex', 'BsmtCond_TA', 'BsmtExposure_Gd', 'BsmtFinType1_ALQ', 'BsmtFinType1_GLQ', 'BsmtFinType2_Unf', 'Heating_GasW', 'HeatingQC_Ex', 'HeatingQC_Gd', 'Electrical_FuseA', 'KitchenQual_Ex', 'Functional_Typ', 'GarageType_Attchd', 'GarageType_Detchd', 'GarageQual_Gd', 'GarageCond_TA', 'PavedDrive_Y', 'MoSold_5', 'MoSold_6', 'MoSold_7', 'SaleCondition_Normal']\n\nFEATURES USED TO SPLIT RF: \n [('OverallQual', 0.6745105446686328), ('TotalBsmtSF', 0.11793511119380447), ('BedroomAbvGr', 0.05364334820501179), ('BsmtFinSF1', 0.05047012060861986), ('KitchenAbvGr', 0.02316110382213497), ('BsmtUnfSF', 0.020974033104939116), ('YearBuilt', 0.01759840709655316), ('OverallCond', 0.008968343902463498), ('HalfBath', 0.005876121408600062), ('YearRemodAdd', 0.00512987722675191), ('LotArea', 0.003964618164158608), ('SaleCondition', 0.0031981633832558433), ('SaleType', 0.0018517043617065135), ('2ndFlrSF', 0.0014229538256023905), ('Fireplaces', 0.0013287664087538772), ('EnclosedPorch', 0.0012812132888585948), ('Heating', 0.0012804264324953745), ('Exterior2nd', 0.0008985239818333273), ('MasVnrArea', 0.0007685312089115593), ('FullBath', 0.0006548546194065412), ('PavedDrive', 0.0005584794256213423), ('MasVnrType', 0.0003770113298907748), ('TotRmsAbvGrd', 0.0003526083965300474), ('1stFlrSF', 0.00029148551341360776), ('BsmtFullBath', 0.00028358952534178483), ('Neighborhood', 0.00017990792656069152), ('GrLivArea', 0.00016811224261091253), ('BsmtExposure', 0.0001475900207264057), ('CentralAir', 0.00013168491910472838), ('BsmtFinType1', 0.00013067240704968774), ('GarageCars', 0.00011622926498471435), ('Foundation', 9.45383443412762e-05), ('RoofStyle', 8.004855912936411e-05), ('HeatingQC', 4.997428238730032e-05), ('GarageCond', 4.392269848508522e-05), ('BsmtQual', 3.816154883380303e-05), ('GarageArea', 3.7758518118270774e-05), ('Electrical', 3.358923068303841e-05), ('ExterQual', 2.155134441578867e-05), ('BsmtCond', 1.7807067897884334e-05), ('GarageFinish', 1.5993562303963467e-05), ('WoodDeckSF', 1.4343578097247227e-05)]\n"
    }
   ],
   "source": [
    "print('LASSO REGRESSION FEATURES SELECTED: \\n', good_features)\n",
    "print()\n",
    "print('FEATURES USED TO SPLIT RF: \\n', rf_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running our analysis we can see very similar features being used to split our decision trees, and the features selected by the LASSO regression model. How about we try to do feature selection using the lasso regression?\n",
    "\n",
    "We may also want to see the difference between using different encodings for our random forest model - whether we want to use label encoding or use one hot encoding. See below: it seems like using label encoding significantly improves the performance of your model because the MSE is lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning of RF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the OOB score to evaluate the random forest model gives us an extremely high OOB r^2 score. \n",
    "Based on the model, we also see that many features have very low importance, and this incentivizes us to revisit the variable selection methodology that we adopted earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try hyperparameter search using random hyperparameter grid, so that we can find optimum hyperparamters for our random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
    }
   ],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num =10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10,110,num =11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2,5,10]\n",
    "min_samples_leaf = [1,2,4]\n",
    "bootstrap = [True]\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   14.0s\n[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:  3.3min finished\nTHIS IS THE BEST SCORE\n0.1393654602033622\nTHIS IS THE BEST PARAMS\n{'n_estimators': 800, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20, 'bootstrap': False}\n"
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 25, cv =5, verbose = 2, random_state = 42, n_jobs = -1, scoring = 'neg_root_mean_squared_error')\n",
    "rf_random.fit(new_feature_set, y)\n",
    "print('THIS IS THE BEST SCORE')\n",
    "print(-rf_random.best_score_)\n",
    "print('THIS IS THE BEST PARAMS')\n",
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 25, cv =5, verbose = 2, random_state = 42, n_jobs = -1, scoring = 'neg_root_mean_squared_error')\n",
    "rf_random.fit(label_X, y)\n",
    "print('THIS IS THE BEST SCORE')\n",
    "print(-rf_random.best_score_)\n",
    "print('THIS IS THE BEST PARAMS')\n",
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We narrowed down the scope of the analysis. We can now adopt grid search with cross validation to do a more in depth search of the optimum hyperparameters\n",
    "\n",
    "Using our paramaters, we derived CV prediction error of 0.13586, based on 5 fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {\n",
    "    'max_depth': [20,25,15],\n",
    "    'min_samples_leaf': [1],\n",
    "    'min_samples_split': [2],\n",
    "    'max_features' : ['sqrt'],\n",
    "    'n_estimators': [800,850, 750]\n",
    "#     'bootstrap': [False]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 5, n_jobs= -1, verbose = 2, scoring = 'neg_root_mean_squared_error')\n",
    "grid_search.fit(X,y)\n",
    "# print(grid_search.best_params_)\n",
    "print('SCORE FOR RF')\n",
    "print(-grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other concerns and questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Incorporating variable selection into our model \n",
    "2. On the internet, everybody is approaching with validation set approach so that they can check whether there is overfitting. How can we check for overfitting when we use a random forest model?\n",
    "3. Why do people still use the validation set approach when using cross validation is supposed to help with this? You check across the different parameters to see which set of parameters gives you the lowest CV error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('3.7.6': pyenv)",
   "language": "python",
   "name": "python37664bit376pyenv016fc51dfd1643ccadebf36b5d805c09"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}